<h2>AWS re:Invent 2019: A day in the life of a Netflix engineer (NFX202)</h2>

<h3>Please note these are rough notes</h3>

| Slides        | Link to Video |
| ------------- |:-------------:|
| TBC           | [link](https://www.youtube.com/watch?v=0QS1TWLooo0) |


<h2> Needs to be formatted</h2>

<h3><u>Neflix Global Presance and how they do it</h3></u>

* 3 regions (eu-west-1, us-west and us-east)
* All databases replicated (within milliseconds)
* All caches hot, all the time

<h3><u>Stack</h3></u>

* Zuul (Layer 7 Proxy Gateway)
    * has retry logic, cross region and rate limiting
* Titus - Container management platform
* Persistence layer (Cassandra)
* Batch and offline containers/jobs etc

<h3><u>Stats</h3></u>

* Elastic Load Balancers - 100s of millions of rps, consistantly
* Zuul - Millions of RPS, conistantly
* 80 million web socket connections open all the time 
* 50k messages per second, all the time
* 3500 ec2 instances GLOBALLY 

<h3><u>Zuul</h3></u>

* Region evacuation exercises
    * Takes a few minutes to migrate all traffic to another aws region
    * Zuul handles this
    * Practice this three times a month

<h3><u>Titus</h3></u>

* 3 million containers deployed via Titus per week
* 50/50 between online or offline and batch containers
* Most containers run between minutes to 1 hour
* Some containers run for days, some containers run for seconds 
* Can launch a container in less than 10 seconds (Recieving traffic), but normally it's less than a second.


<h3><u>Cassandra Cluster</h3></u>

* 100PB+
* 10,000 nodes
* Non durable writes take less than 1ms to replicate across the cluster
* Trillions of records


<h3><u>Other</h3></u>

* Use Kafka
* Use RDS family


<h3></u>Netflix Open Connect Applicance</h3></u>

* Physical units that amazon deploy in ISPs and their own PoPs
    * SSD, Flash and HDDs in order to put data where it's most popular
    * 100Gbps networking
    * FreeBSD
    * NGINX frontend
* One 'stack' of OCAs (20 OCAs) can manage 1% of the global internet traffic
* Have their own network backbone to the OCAs to be able to seed them without affecting the ISP

<h3><u>Netflix imagery</h3></u>

* Billions of images, whole catalog took around 7 weeks to regenerate before Dynaitation
* Dynatitation is a continious ML using layering.
* Dynatitation takes 20 minutes to regenerate the whole catalog
    * Results in immidate feedback, no waste

<h3><u>Netflix's Network Challenge</h3></u>

* Always will have a last mile challenge
* Redirect via their own backbone when ISPs are having issues
* Four modes
    * ISP -> AWS
    * OCA -> ISP Backbone -> AWS
    * OCA -> Netflix Backbone -> AWS
    * OCA -> Backbone OCA -> AWS

<h3><u>Netflix tooling</h3></u>

* Spinnaker
    * Any team can create canary analysis. Matches behaviours against current version.
    * Loads of details - how everything works on the service etc

* App owners
    * Focus on requirements (public , this much cpu. Platform team get requirements not done by app owners)

* Stats
    * 1000s of deployments per day
    * 50000 pipelines everyday (millions of tasks)

<h3><u>Netflix Observability</h3></u>

* As complexity increases the less you know

* Netflix's Mantra
    * Dont limit information 
    * Fast
    * Broad coverage
    * Cost effective 

* Mantis
    * 5 billion times series metrics.
    * Event streaming and processing
    * High volume - anolomy detection etc
    * streaming health
    * contextual healing
    * Cassandra health checks 
    * chaos experimentation monitoring 
    * PII detection
